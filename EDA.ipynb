{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.decomposition import NMF,LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "us = pickle.load(open(\"wikivoyage_text_US.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(us,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns=({ 'index' : 'Name'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AbercrombiePAGE</td>\n",
       "      <td>{{pagebanner|Abercrombie WikiVoyage Banner ND....</td>\n",
       "      <td>page</td>\n",
       "      <td>//tools.wmflabs.org/wikivoyage/w/poimap2.php?l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aberdeen (Maryland)PAGE</td>\n",
       "      <td>{{pagebanner|Aberdeen MD WikiVoyage Banner.jpg...</td>\n",
       "      <td>page</td>\n",
       "      <td>//tools.wmflabs.org/wikivoyage/w/poimap2.php?l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aberdeen (South Dakota)PAGE</td>\n",
       "      <td>{{pagebanner|Pagebanner default.jpg|pgname=Abe...</td>\n",
       "      <td>page</td>\n",
       "      <td>//tools.wmflabs.org/wikivoyage/w/poimap2.php?l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aberdeen (Washington)PAGE</td>\n",
       "      <td>{{Pagebanner|pgname=Aberdeen |Wikivoyage page ...</td>\n",
       "      <td>page</td>\n",
       "      <td>//tools.wmflabs.org/wikivoyage/w/poimap2.php?l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AbernathyPAGE</td>\n",
       "      <td>{{pagebanner|Abernathy Texas Wikivoyage Banner...</td>\n",
       "      <td>page</td>\n",
       "      <td>//tools.wmflabs.org/wikivoyage/w/poimap2.php?l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Name  \\\n",
       "0              AbercrombiePAGE   \n",
       "1      Aberdeen (Maryland)PAGE   \n",
       "2  Aberdeen (South Dakota)PAGE   \n",
       "3    Aberdeen (Washington)PAGE   \n",
       "4                AbernathyPAGE   \n",
       "\n",
       "                                                text  type  \\\n",
       "0  {{pagebanner|Abercrombie WikiVoyage Banner ND....  page   \n",
       "1  {{pagebanner|Aberdeen MD WikiVoyage Banner.jpg...  page   \n",
       "2  {{pagebanner|Pagebanner default.jpg|pgname=Abe...  page   \n",
       "3  {{Pagebanner|pgname=Aberdeen |Wikivoyage page ...  page   \n",
       "4  {{pagebanner|Abernathy Texas Wikivoyage Banner...  page   \n",
       "\n",
       "                                                 loc  \n",
       "0  //tools.wmflabs.org/wikivoyage/w/poimap2.php?l...  \n",
       "1  //tools.wmflabs.org/wikivoyage/w/poimap2.php?l...  \n",
       "2  //tools.wmflabs.org/wikivoyage/w/poimap2.php?l...  \n",
       "3  //tools.wmflabs.org/wikivoyage/w/poimap2.php?l...  \n",
       "4  //tools.wmflabs.org/wikivoyage/w/poimap2.php?l...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unwiki(wiki):\n",
    "        \"\"\"\n",
    "       Remove wiki markup from the text.\n",
    "       \"\"\"\n",
    "        wiki = re.sub(r'(?i)\\{\\{IPA(\\-[^\\|\\{\\}]+)*?\\|([^\\|\\{\\}]+)(\\|[^\\{\\}]+)*?\\}\\}', lambda m: m.group(2), wiki)\n",
    "        wiki = re.sub(r'(?i)\\{\\{Lang(\\-[^\\|\\{\\}]+)*?\\|([^\\|\\{\\}]+)(\\|[^\\{\\}]+)*?\\}\\}', lambda m: m.group(2), wiki)\n",
    "        wiki = re.sub(r'\\{\\{[^\\{\\}]+\\}\\}', '', wiki)\n",
    "        wiki = re.sub(r'(?m)\\{\\{[^\\{\\}]+\\}\\}', '', wiki)\n",
    "        wiki = re.sub(r'(?m)\\{\\|[^\\{\\}]*?\\|\\}', '', wiki)\n",
    "        wiki = re.sub(r'(?i)\\[\\[Category:[^\\[\\]]*?\\]\\]', '', wiki)\n",
    "        wiki = re.sub(r'(?i)\\[\\[Image:[^\\[\\]]*?\\]\\]', '', wiki)\n",
    "        wiki = re.sub(r'(?i)\\[\\[File:[^\\[\\]]*?\\]\\]', '', wiki)\n",
    "        wiki = re.sub(r'\\[\\[[^\\[\\]]*?\\|([^\\[\\]]*?)\\]\\]', lambda m: m.group(1), wiki)\n",
    "        wiki = re.sub(r'\\[\\[([^\\[\\]]+?)\\]\\]', lambda m: m.group(1), wiki)\n",
    "        wiki = re.sub(r'\\[\\[([^\\[\\]]+?)\\]\\]', '', wiki)\n",
    "        wiki = re.sub(r'(?i)File:[^\\[\\]]*?', '', wiki)\n",
    "        wiki = re.sub(r'\\[[^\\[\\]]*? ([^\\[\\]]*?)\\]', lambda m: m.group(1), wiki)\n",
    "        wiki = re.sub(r\"''+\", '', wiki)\n",
    "        wiki = re.sub(r'(?m)^\\*$', '', wiki)\n",
    "       \n",
    "        return wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unhtml(html):\n",
    "        \"\"\"\n",
    "       Remove HTML from the text.\n",
    "       \"\"\"\n",
    "        html = re.sub(r'(?i)&nbsp;', ' ', html)\n",
    "        html = re.sub(r'(?i)<br[ \\\\]*?>', '\\n', html)\n",
    "        html = re.sub(r'(?m)<!--.*?--\\s*>', '', html)\n",
    "        html = re.sub(r'(?i)<ref[^>]*>[^>]*<\\/ ?ref>', '', html)\n",
    "        html = re.sub(r'(?m)<.*?>', '', html)\n",
    "        html = re.sub(r'(?i)&amp;', '&', html)\n",
    "       \n",
    "        return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return unhtml(unwiki(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\", max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_features = vect.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=20, random_state=1, alpha=0, l1_ratio=0)\n",
    "W = nmf.fit_transform(x_features)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_features = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print \"Topic #%d:\" % topic_idx\n",
    "        print \" \".join(sorted([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]],key=lambda x: len(x),reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_top_words(nmf, vector_features, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for topic in range(W.shape[1]):\n",
    "    #max_t = np.argmax(W[:,topic])\n",
    "    print \"TOPIC: #\",topic\n",
    "    #print features[max_t][0:150]\n",
    "    \n",
    "    for t in range(10):\n",
    "        n = int(-1-t)\n",
    "        max_2t = W[:,topic].argsort()[n]\n",
    "        print df['Name'][max_2t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(df[df['Name']=='Touring prestigious and notable universities in the U.S.PAGE']['loc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pages = df[df['type'] == 'page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return re.sub('PAGE', '', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "pages['Name'] = pages['Name'].apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(\"//tools.wmflabs.org/wikivoyage/w/poimap2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pages = pages[pages['loc'].str.contains(\"//tools.wmflabs.org/wikivoyage/w/poimap2\")==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "StopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "StopPlaces = list(pages['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "StopPlaces = map(lambda x: re.sub(\"\\(|\\)\", '',x), StopPlaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "StopPlaces = map(lambda x: x.lower().split(), StopPlaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "StopPlaces = [item for sublist in StopPlaces for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "StopWords = StopPlaces + StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "StopWords = list(set(StopWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words=StopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = pages['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_features = vect.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=20, random_state=1, alpha=0, l1_ratio=0)\n",
    "W = nmf.fit_transform(x_features)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_features = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_top_words(nmf, vector_features, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for topic in range(W.shape[1]):\n",
    "    #max_t = np.argmax(W[:,topic])\n",
    "    print \"TOPIC: #\",topic\n",
    "    #print features[max_t][0:150]\n",
    "    \n",
    "    for t in range(10):\n",
    "        n = int(-1-t)\n",
    "        max_2t = W[:,topic].argsort()[n]\n",
    "        print list(pages['Name'])[max_2t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "StopWords = StopWords + map(lambda x: x.split(\"/\"), StopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "StopWords = [item for sublist in StopWords for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TFIDFvect = TfidfVectorizer(stop_words=StopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_features = TFIDFvect.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=20, random_state=1, alpha=0, l1_ratio=0)\n",
    "W = nmf.fit_transform(x_features)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_features = TFIDFvect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_top_words(nmf, vector_features, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for topic in range(W.shape[1]):\n",
    "    #max_t = np.argmax(W[:,topic])\n",
    "    print \"TOPIC: #\",topic\n",
    "    #print features[max_t][0:150]\n",
    "    \n",
    "    for t in range(10):\n",
    "        n = int(-1-t)\n",
    "        max_2t = W[:,topic].argsort()[n]\n",
    "        print list(pages['Name'])[max_2t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=20, random_state=1)\n",
    "W = lda.fit_transform(x_features)\n",
    "H = lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_top_words(lda,vector_features, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for topic in range(W.shape[1]):\n",
    "    #max_t = np.argmax(W[:,topic])\n",
    "    print \"TOPIC: #\",topic\n",
    "    #print features[max_t][0:150]\n",
    "    \n",
    "    for t in range(10):\n",
    "        n = int(-1-t)\n",
    "        max_2t = W[:,topic].argsort()[n]\n",
    "        print list(pages['Name'])[max_2t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pages[pages['Name'] == 'Buffalo/East Side']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print len(pages['text'].ix[801])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x = re.sub(r'WikiPedia:([\\s\\S]*)','', x)\n",
    "    x = re.sub(r'Dmoz:([\\s\\S]*)','', x)\n",
    "    x = re.sub(r'\\[([\\s\\S]*)\\]','', x)\n",
    "    x = re.sub(r'[0-9]','', x)\n",
    "    x = re.sub(r'\\=\\=Get in\\=\\=([\\s\\S]*)(?=\\=\\=Get around\\=\\=)','',x)\n",
    "    x = re.sub(r'\\=\\=Get around\\=\\=([\\s\\S]*)(?=\\=\\=See\\=\\=)','',x)\n",
    "    x = re.sub(r'\\=\\=([\\S ]*)\\=\\=', '', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pages['regex_text'] = pages['text'].apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pages['len'] = map(lambda x: len(x), pages['regex_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pages2 = pages[pages['len']> 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(pages2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = pages2['regex_text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TFIDFvect = TfidfVectorizer(stop_words=StopWords)\n",
    "x_features = TFIDFvect.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=10, random_state=1, alpha=0, l1_ratio=0)\n",
    "W = nmf.fit_transform(x_features)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector_features = TFIDFvect.get_feature_names()\n",
    "\n",
    "print_top_words(nmf, vector_features, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for topic in range(W.shape[1]):\n",
    "    #max_t = np.argmax(W[:,topic])\n",
    "    print \"TOPIC: #\",topic\n",
    "    #print features[max_t][0:150]\n",
    "    \n",
    "    for t in range(10):\n",
    "        n = int(-1-t)\n",
    "        max_2t = W[:,topic].argsort()[n]\n",
    "        print list(pages2['Name'])[max_2t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words=StopWords,ngram_range=(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_features = vect.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=10, random_state=1, alpha=0, l1_ratio=0)\n",
    "W = nmf.fit_transform(x_features)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "neighborhoods restaurants especially expensive including listings although detailed largest chinese popular yorkers around famous places subway public people stores within nearby though every pages major often large times music night small known found many also find well food bars good take like one art get see nyc bus go th\n",
      "Topic #1:\n",
      "mdash preserves set mdash commemorates revolution mdash mdash preserves president mdash mdash contains preserves set mdash former commemorates spectacular revolution usfs mdash th century preserves including president blm mdash mdash set contains numerous situated century several largest british ancient former famous system native dating mdash civil sites first major noted along famed built years usfs well many ride set blm one th st\n",
      "Topic #2:\n",
      "reservations restaurants attractions restaurant available boardwalk property children fastpass resorts service guests dining though around hotels people mdash theme rides night check align also time even find many kids want take like ride room food make need cast well free days one get fun use car see re ll go\n",
      "Topic #3:\n",
      "neighborhoods hertel avenue neighborhood restaurants beginning community pizzerias broadway nfta bus parkside proceeds adjacent streets parking located olmsted parkway century transit however avenue though hertel around system ending along first still local today years areas crime urban well also nfta rail line road part much like bike best bus via one re\n",
      "Topic #4:\n",
      "backcountry mi km trip entrance although moderate required km trip visitor panther several chisos trails hiking marked nearby mi km mdash along drive views first sites route leads least heads elena road trip also turn take well even hike easy good sure like time make away one see get mi km ll tx re\n",
      "Topic #5:\n",
      "neighborhoods neighborhood restaurants th avenue buildings broadway building streets avenue around famous subway th st along st th th th lines times route music night st st major shops many also line take runs bars find best stop good like walk well food free ave one get see bus art th st rd nd go\n",
      "Topic #6:\n",
      "restaurants restaurant available including beautiful breakfast shopping features festival located largest popular minutes offers stores family summer school campus dining small local along drive large shops pizza known route store enjoy also many food road open well best shop good free take one inn ave art pm pa rd st\n",
      "Topic #7:\n",
      "neighborhood restaurants attractions th century community visitors cultural suburban articles located largest century include despite usually though around events public places number locals avenue local first still among years found small music class scene known whose based well also many best like time much take year one see art fm th\n",
      "Topic #8:\n",
      "restaurants especially including available visitors although popular several however largest usually weather around people summer trails hiking common within areas local along large known often small major first found offer visit still years many also well even time like year much good part find best take food one see get\n",
      "Topic #9:\n",
      "neighborhoods neighborhood restaurants throughout buildings although african museums several summer polish sports though public second places campus around blues known still major along every field local pizza music also well like best good bars many bike jazz even time free line beef play away make one see art cta re\n"
     ]
    }
   ],
   "source": [
    "vector_features = vect.get_feature_names()\n",
    "\n",
    "print_top_words(nmf, vector_features, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC: # 0\n",
      "New York City\n",
      "Manhattan\n",
      "San Francisco\n",
      "Metro New York\n",
      "Philadelphia\n",
      "Seattle\n",
      "Pittsburgh\n",
      "Los Angeles\n",
      "Atlanta\n",
      "Brooklyn\n",
      "TOPIC: # 1\n",
      "United States National Parks\n",
      "Disneyland\n",
      "Finger Lakes\n",
      "Montana\n",
      "Early United States history\n",
      "Rochester and Suburbs\n",
      "San Francisco/Golden Gate\n",
      "Rochester (New York)\n",
      "Atlanta\n",
      "Orleans County (New York)\n",
      "TOPIC: # 2\n",
      "Walt Disney World\n",
      "Walt Disney World/Magic Kingdom\n",
      "Walt Disney World/Epcot\n",
      "Disneyland\n",
      "Walt Disney World/Disney Springs\n",
      "Universal Orlando\n",
      "Walt Disney World/Animal Kingdom\n",
      "Walt Disney World/Hollywood Studios\n",
      "O'Hare International Airport\n",
      "Las Vegas\n",
      "TOPIC: # 3\n",
      "Buffalo/East Side\n",
      "Buffalo/South Buffalo\n",
      "Buffalo/North Buffalo\n",
      "Buffalo/West Side\n",
      "Buffalo/Elmwood Village\n",
      "Buffalo/Allentown and the Delaware District\n",
      "Buffalo/Downtown\n",
      "Brooklyn\n",
      "Minneapolis/South\n",
      "Chicago/Far Northwest Side\n",
      "TOPIC: # 4\n",
      "Big Bend National Park\n",
      "Bandelier National Monument\n",
      "Black Canyon of the Gunnison National Park\n",
      "Four Corners\n",
      "Isle Royale National Park\n",
      "North Central New Mexico\n",
      "Clarkston (Michigan)\n",
      "Santa Fe (New Mexico)\n",
      "Buffalo/South Buffalo\n",
      "Yosemite National Park\n",
      "TOPIC: # 5\n",
      "Manhattan\n",
      "Washington, D.C./Shaw\n",
      "Manhattan/Upper West Side\n",
      "Brooklyn\n",
      "Manhattan/Harlem and Upper Manhattan\n",
      "Manhattan/Theater District\n",
      "New Orleans\n",
      "Manhattan/Midtown East\n",
      "Chicago/Hyde Park\n",
      "Washington, D.C./East End\n",
      "TOPIC: # 6\n",
      "Lansing (Michigan)\n",
      "Kennett Square\n",
      "East Lansing\n",
      "Philadelphia\n",
      "Shrewsbury (Pennsylvania)\n",
      "Chincoteague\n",
      "New Milford\n",
      "Selinsgrove\n",
      "Lewes (Delaware)\n",
      "Washington, D.C.\n",
      "TOPIC: # 7\n",
      "Buffalo\n",
      "Rochester (New York)\n",
      "Charlotte\n",
      "Philadelphia\n",
      "Buffalo/South Buffalo\n",
      "Pittsburgh\n",
      "Seattle\n",
      "Buffalo/Downtown\n",
      "Atlanta\n",
      "Detroit\n",
      "TOPIC: # 8\n",
      "Hawaii\n",
      "San Francisco\n",
      "New Mexico\n",
      "Colorado\n",
      "New Orleans\n",
      "Seattle\n",
      "San Diego\n",
      "California\n",
      "Yosemite National Park\n",
      "Washington, D.C.\n",
      "TOPIC: # 9\n",
      "Chicago\n",
      "Washington, D.C.\n",
      "San Francisco\n",
      "Apple Valley (Minnesota)\n",
      "Baltimore\n",
      "Pittsburgh\n",
      "Chicago/Loop\n",
      "Atlanta\n",
      "Minneapolis\n",
      "Los Angeles\n"
     ]
    }
   ],
   "source": [
    "for topic in range(W.shape[1]):\n",
    "    #max_t = np.argmax(W[:,topic])\n",
    "    print \"TOPIC: #\",topic\n",
    "    #print features[max_t][0:150]\n",
    "    \n",
    "    for t in range(10):\n",
    "        n = int(-1-t)\n",
    "        max_2t = W[:,topic].argsort()[n]\n",
    "        print list(pages2['Name'])[max_2t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Consider breaking text into sections (e.g. DO, EAT, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Remove get in/ get around sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Try POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Consider NER for names of restaurants, hotels, etc. so "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Consider removing airports\n",
    "## Combine city sections into one\n",
    "## Remove National parks and history pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Add European Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
