{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec On The WikiVoyage Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivation: I love to travel, especially to offbeat/out-of-the-way/unusual places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge: Discovering these places is still a highly manual process. \n",
    "\n",
    "Currently relies on:\n",
    "\n",
    "- Folk knowledge\n",
    "- Word of mouth (hopefully your friends have some good recommendations)\n",
    "- Random discovery (reading the right article, watching the right travel show)\n",
    "- Wandering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: To develop a system to help people discover new places to travel to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How: Use the Wikivoyage corpus (currently only North America and Europe) to train a Doc2Vec model and then use cosine similarity to generate new travel ideas based on places and topics that we feed into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Doc2Vec.load(\"doc2vec_dm_NAEUR.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Las Vegas', 0.4692399799823761),\n",
       " (u'Wells (Nevada)', 0.44553759694099426),\n",
       " (u'Black Hawk', 0.41475480794906616),\n",
       " (u'Pahrump', 0.40906670689582825),\n",
       " (u'Beatty', 0.40621137619018555),\n",
       " (u'Boulder City', 0.4015314280986786),\n",
       " (u'Laughlin', 0.3741351068019867),\n",
       " (u'Atlantic City', 0.3719399571418762),\n",
       " (u'Lake Tahoe', 0.3686116337776184),\n",
       " (u'South Lake Tahoe', 0.35805609822273254)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar(positive=[model[\"gambling\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Lake Tahoe', 0.42162561416625977),\n",
       " (u'South Lake Tahoe', 0.41278210282325745),\n",
       " (u'Rauland', 0.39757058024406433),\n",
       " (u'Wells (Nevada)', 0.35675251483917236),\n",
       " (u'Las Vegas', 0.3492353856563568),\n",
       " (u'Nevada', 0.34744125604629517),\n",
       " (u'Reno', 0.34133267402648926),\n",
       " (u'Yll\\xe4s', 0.3409240245819092),\n",
       " (u'Black Hawk', 0.33933377265930176),\n",
       " (u\"Val-d'Is\\xe8re\", 0.33322441577911377)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar(positive=[model[\"skiing\"],model[\"gambling\"],\"California\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Mayan Riviera', 0.45207858085632324),\n",
       " (u'Huatulco', 0.43176454305648804),\n",
       " (u'Zipolite', 0.42858099937438965),\n",
       " (u'Ixtapa', 0.4270773231983185),\n",
       " (u'Akumal', 0.4214611053466797),\n",
       " (u'Cozumel', 0.4185025095939636),\n",
       " (u'Playa del Carmen', 0.4122311472892761),\n",
       " (u'Zihuatanejo', 0.4080858826637268),\n",
       " (u'Puerto Vallarta', 0.40323808789253235),\n",
       " (u'Vasto', 0.4008360207080841)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar(positive=[model[\"beaches\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Walt Disney World/Disney Springs', 0.31546011567115784),\n",
       " (u'Walt Disney World', 0.31259435415267944),\n",
       " (u'Varvara', 0.3068995773792267),\n",
       " (u'Canc\\xfan', 0.30667930841445923),\n",
       " (u'Looe', 0.29569607973098755),\n",
       " (u'Walt Disney World/Hollywood Studios', 0.29355520009994507),\n",
       " (u'Lappaj\\xe4rvi', 0.2845228910446167),\n",
       " (u'Walt Disney World/Animal Kingdom', 0.2813645899295807),\n",
       " (u'Acapulco', 0.2766570448875427),\n",
       " (u'Kuortane', 0.2747095227241516)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar(positive=[\"Las Vegas\",model['kids'],\"Florida\"],negative=[\"Nevada\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Atlanta', 0.39574217796325684),\n",
       " (u'Cookeville', 0.27874839305877686),\n",
       " (u'Greenville (South Carolina)', 0.2569996118545532),\n",
       " (u'Wilson', 0.24723148345947266),\n",
       " (u'Greenville (North Carolina)', 0.24619674682617188),\n",
       " (u'Research Triangle', 0.23729246854782104),\n",
       " (u'Brunswick (Georgia)', 0.23575204610824585),\n",
       " (u'Piedmont (Georgia)', 0.23453886806964874),\n",
       " (u'Seattle/Downtown', 0.22976917028427124),\n",
       " (u'Seattle/Queen Anne-South Lake Union', 0.22697080671787262)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar(positive=[\"Seattle\",\"Georgia (state)\"],negative=[\"Washington (state)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Something more interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who knows of Williamsburg in Brooklyn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert picture of Fat Jewish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historically a working class area - subsequently taken over by yuppies and hipsters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your views, you may want to visit places like this (or run from them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the Williamsburg locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Williamsburg(Metro):\n",
    "    return model.docvecs.most_similar(positive=[\"Brooklyn/Williamsburg\",Metro],negative=[\"New York City\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'San Francisco/SoMa', 0.34045329689979553),\n",
       " (u'Oakland', 0.31408262252807617),\n",
       " (u'San Francisco/Castro-Noe Valley', 0.30932530760765076),\n",
       " (u'San Francisco/The Avenues', 0.3084043860435486),\n",
       " (u'San Francisco/Western Addition', 0.30720651149749756),\n",
       " (u'San Francisco/Southeast', 0.3040141761302948),\n",
       " (u'San Jose (California)', 0.29996657371520996),\n",
       " (u'San Francisco/Golden Gate', 0.2990891933441162),\n",
       " (u'Millbrae', 0.2954830825328827),\n",
       " (u'San Mateo', 0.2934788465499878)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Williamsburg(\"San Francisco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Cambridge (Massachusetts)', 0.3884972631931305),\n",
       " (u'Boston/Dorchester', 0.2815379202365875),\n",
       " (u'Boston/Beacon Hill', 0.25701427459716797),\n",
       " (u'Nashua (New Hampshire)', 0.25510767102241516),\n",
       " (u'Newton', 0.2438473403453827),\n",
       " (u'Boston/Jamaica Plain', 0.24315953254699707),\n",
       " (u'New Bedford', 0.24228599667549133),\n",
       " (u'Boston/Charlestown', 0.23969599604606628),\n",
       " (u'Lexington (Massachusetts)', 0.23508819937705994),\n",
       " (u'Miami/Overtown', 0.23371116816997528)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Williamsburg(\"Boston\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Paris/20th arrondissement', 0.356248676776886),\n",
       " (u'Paris/19th arrondissement', 0.3258187472820282),\n",
       " (u'Paris/6th arrondissement', 0.3206578195095062),\n",
       " (u'Paris/17th arrondissement', 0.3003493845462799),\n",
       " (u'Nogent-sur-Marne', 0.2983584702014923),\n",
       " (u'Paris/10th arrondissement', 0.2917677164077759),\n",
       " (u'Paris/9th arrondissement', 0.2859533429145813),\n",
       " (u'Paris/La D\\xe9fense', 0.280295729637146),\n",
       " (u'Saint Denis', 0.26104071736335754),\n",
       " (u'Paris/12th arrondissement', 0.25988340377807617)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Williamsburg(\"Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Montreal/Quartier Latin-Le Village', 0.341674268245697),\n",
       " (u'Montreal/South West', 0.30806463956832886),\n",
       " (u'Quebec City', 0.2635706961154938),\n",
       " (u'Montreal/West Island', 0.26224732398986816),\n",
       " (u'Quebec', 0.25512123107910156),\n",
       " (u'Sherbrooke', 0.25188568234443665),\n",
       " (u'Montreal/Downtown', 0.250841349363327),\n",
       " (u'Mont\\xe9r\\xe9gie', 0.24994179606437683),\n",
       " (u'Montreal/Hochelaga-Maisonneuve', 0.23459258675575256),\n",
       " (u'Rigaud', 0.22433531284332275)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Williamsburg(\"Montreal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Detroit/Downtown', 0.42757827043533325),\n",
       " (u'Metro Detroit', 0.3878357410430908),\n",
       " (u'Detroit/Midtown-New Center', 0.3778426945209503),\n",
       " (u'Franklin (Michigan)', 0.2981802523136139),\n",
       " (u'Detroit/Southwest Side', 0.29475998878479004),\n",
       " (u'Washtenaw County', 0.28032079339027405),\n",
       " (u'Ann Arbor', 0.2722225785255432),\n",
       " (u'Flint', 0.26905524730682373),\n",
       " (u'Dearborn', 0.26677072048187256),\n",
       " (u'Ypsilanti', 0.24946075677871704)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Williamsburg('Detroit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Atlanta/Little Five Points', 0.36603689193725586),\n",
       " (u'Cleveland/West Side', 0.3568888008594513),\n",
       " (u'Manhattan/Lower East Side', 0.34965312480926514),\n",
       " (u'Paris/20th arrondissement', 0.3441842794418335),\n",
       " (u'Seattle/Capitol Hill-Central District', 0.33608317375183105),\n",
       " (u'Buffalo/Allentown and the Delaware District', 0.3348085880279541),\n",
       " (u'Queens/Long Island City and Astoria', 0.3264613747596741),\n",
       " (u'Buffalo/West Side', 0.3235052525997162),\n",
       " (u'Chicago/Wicker Park', 0.31541532278060913),\n",
       " (u'San Francisco/Haight', 0.30540892481803894),\n",
       " (u'Paris/17th arrondissement', 0.30482929944992065),\n",
       " (u'Boston/South End', 0.29873865842819214),\n",
       " (u'Berlin/East Central', 0.29147371649742126),\n",
       " (u'Buffalo/North Buffalo', 0.2911246120929718),\n",
       " (u'Buffalo/Elmwood Village', 0.2910820245742798),\n",
       " (u'Chicago/Lincoln Park-Old Town', 0.283674955368042),\n",
       " (u'San Francisco/Castro-Noe Valley', 0.2836647033691406),\n",
       " (u'San Francisco/Civic Center-Tenderloin', 0.2829035520553589),\n",
       " (u'San Francisco/Mission', 0.2633632719516754),\n",
       " (u'Hamburg/Altona-St. Pauli', 0.26288843154907227),\n",
       " (u'Chicago/Pilsen', 0.2606809139251709),\n",
       " (u'Chicago/Lakeview-North Center', 0.2594261169433594),\n",
       " (u'Stockholm/Norrmalm', 0.2565544545650482),\n",
       " (u'Washington, D.C./Near Northeast', 0.2556825876235962),\n",
       " (u'Houston/Neartown', 0.2541565001010895),\n",
       " (u'Buffalo/Downtown', 0.2521442770957947),\n",
       " (u'San Francisco/Bernal Heights', 0.25205451250076294),\n",
       " (u'Chicago/Bridgeport-Chinatown', 0.2515241503715515),\n",
       " (u'Chicago/Far Southwest Side', 0.24653293192386627),\n",
       " (u'Toronto/Kensington-Chinatown', 0.24489910900592804),\n",
       " (u'North Hollywood', 0.24389883875846863),\n",
       " (u'Chicago/Logan-Bucktown', 0.24302344024181366),\n",
       " (u'Manhattan/SoHo', 0.2423328012228012),\n",
       " (u'Seattle/Fremont', 0.2407372146844864),\n",
       " (u'Dallas/Uptown', 0.23859024047851562),\n",
       " (u'Austin/UT and the Drag', 0.23811699450016022),\n",
       " (u'Paris/12th arrondissement', 0.23783911764621735),\n",
       " (u'Amsterdam/Jordaan', 0.23761312663555145),\n",
       " (u'Boston/Dorchester', 0.23750311136245728),\n",
       " (u'Charlotte/Elizabeth', 0.23735195398330688),\n",
       " (u'Vienna/Neubau', 0.23693376779556274),\n",
       " (u'Stockholm/S\\xf6dermalm', 0.23583704233169556),\n",
       " (u'Atlanta/Midtown', 0.233628511428833),\n",
       " (u'Providence/College Hill', 0.23359361290931702),\n",
       " (u'Philadelphia/South', 0.2330065667629242),\n",
       " (u'Baltimore/Southeast Baltimore', 0.23247183859348297),\n",
       " (u'Milan/South', 0.23188838362693787),\n",
       " (u'Van Nuys', 0.2314745932817459),\n",
       " (u'Brooklyn/Williamsburg', 0.23128840327262878),\n",
       " (u'San Francisco', 0.2285921573638916)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar([model['hipsters']],topn=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveats\n",
    "- Still requires some human review as sometimes it will generate recommendations that do not make \"human\" sense\n",
    "- Some category pages are still in the dataset, so those probably need to be removed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travel Companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Help customers decide where to travel (e.g. Kayak, Priceline, Orbitz)\n",
    "- If customers already know where they're going, help them decide where to stay/visit (e.g. Homeaway, Airbnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec/Doc2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizes a simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks:\n",
    "\n",
    "1. Take in series of inputs (features)\n",
    "2. Apply initial weights\n",
    "3. Apply hidden layer activation functions\n",
    "4. Apply additional weights\n",
    "5. (Repeat steps 3 and 4 as desired) - NOT USED IN Word2Vec\n",
    "6. Apply Output Function\n",
    "7. Update weights based on Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert simple NN image from video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Net Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pedro Domingos (In his book the Master Algorithm) has a really good example of a simple Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR formulation - returns TRUE if one condition is true. If both or none of the conditions are true, return False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eg. It's said that Nike's customer base consists of 2 groups, young males and older females."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formulation cannot be learned with a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Net is capable of learning an XOR forumulation with 3 nodes in the hidden layer\n",
    "1. Activates if young male\n",
    "2. Activates if older female\n",
    "3. Activates if either of the other two activates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model assumes the Distributional Hypothesis: words that appear together share the same meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of the CBOW model is to maximize the probability of the target word given its context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs: One Hot Encoded Vector representations of words in the vocabulary. Each word is arbritarily assigned a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Layer: Size is determined by user (in this case 300 nodes were used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs are vector representations of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to maximize the probability of an output word given an input context (CBOW) (context window is set by the user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec Continuous Bag of Words Model (CBOW):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Encode words as input vectors\n",
    "2. Take the dot product of the input vectors and the weight matrix W\n",
    "3. Repeat for the number of words in the size of the context C\n",
    "4. At the hidden layer, take the summation of all the dot products from the previous two steps\n",
    "5. Take the dot product of the output of the hidden layer and the second weight matrix, Wprime\n",
    "6. The output function (in this case a softmax function is used) is applied to the output of step 5\n",
    "7. Backprop to update the weights using Gradient Descent\n",
    "8. Repeat for the desired number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Visual representation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec / Paragraph Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec is very similar to Word2Vec and is used to learn the labels/documents for a corpus of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, the Wikivoyage dataset has the names of places (labels) which I will associate with the words (articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Doc2Vec, a token that represents the document is a added to the context in the CBOW model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector representation of that document is then generated and can be used in Cosine similarity calcuations or as an input to another model (e.g. Kmeans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilized the MWClient to import data from a Wikimedia site (in this case Wikivoyage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is stored in a tree structure with pages about a location stored underneath categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially I tried to recursively download the data, but I found that iteratively traversing the tree structure worked better "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import mwclient\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "class getData(object):\n",
    "\n",
    "    def __init__(self,site,pages):\n",
    "        self.site = mwclient.Site(site)\n",
    "        self.pages = self.site.Categories[pages]\n",
    "        self.data = defaultdict(dict)\n",
    "        #self.count = 0\n",
    "\n",
    "    #def create_shelve(self):\n",
    "    #    self.d = shelve.open('wikivoyage.db')\n",
    "\n",
    "\n",
    "    def _fetch_data(self,site):\n",
    "        if type(site) == mwclient.page.Page:\n",
    "            self.data[site.name] = defaultdict(str)\n",
    "            self.data[site.name]['text'] = site.text()\n",
    "            self.data[site.name]['loc'] = site.extlinks().next()\n",
    "\n",
    "\n",
    "        else:\n",
    "            #print self.count\n",
    "            map(self._fetch_data,site.members())\n",
    "\n",
    "    def fetch_data(self):\n",
    "        self._fetch_data(self.pages)\n",
    "\n",
    "    def iterativeChildren(self,nodes):\n",
    "        #from http://blog.nextgenetics.net/?e=64\n",
    "        while 1:\n",
    "            newNodes = []\n",
    "            if len(nodes) == 0:\n",
    "                break\n",
    "            for node in nodes:\n",
    "                if type(node) != mwclient.page.Page:\n",
    "                    newNodes += list(node.members())\n",
    "                    self.data[node.name+\"CAT\"] = defaultdict(str)\n",
    "                    self.data[node.name+\"CAT\"]['text'] = node.text()\n",
    "                    self.data[node.name+\"CAT\"]['type'] = \"category\"\n",
    "                    try:\n",
    "                        self.data[node.name+\"PAGE\"]['loc'] = node.extlinks().next()\n",
    "                    except:\n",
    "                        continue\n",
    "                else:\n",
    "                    self.data[node.name+\"PAGE\"] = defaultdict(str)\n",
    "                    self.data[node.name+\"PAGE\"]['text'] = node.text()\n",
    "                    self.data[node.name+\"PAGE\"]['type'] = \"page\"\n",
    "                    try:\n",
    "                        self.data[node.name+\"PAGE\"]['loc'] = node.extlinks().next()\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            nodes = newNodes\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #reload(sys)  # Reload does the trick!\n",
    "    #sys.setdefaultencoding('UTF8')\n",
    "    gd = getData('en.wikivoyage.org/','United States of America')\n",
    "    gd.iterativeChildren(list(gd.pages.members()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved data about the US, Canada, Mexico and Europe in pickle files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA and Preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Load in data\n",
    "us = pickle.load(open(\"wikivoyage_text_US.p\",\"rb\"))\n",
    "canada = pickle.load(open(\"wikivoyage_text_Canada.p\",\"rb\"))\n",
    "europe = pickle.load(open(\"wikivoyage_text_Europe.p\",\"rb\"))\n",
    "mexico = pickle.load(open(\"wikivoyage_text_Mexico.p\",\"rb\"))\n",
    "\n",
    "# Combine into one dataframe\n",
    "df = pd.DataFrame.from_dict(us,orient='index')\n",
    "df['region'] = 'us'\n",
    "df = df.reset_index()\n",
    "df = df.rename(columns=({ 'index' : 'Name'}))\n",
    "\n",
    "dfsets = [canada,europe,mexico]\n",
    "dfset_names = ['canada','europe','mexico']\n",
    "\n",
    "for dataset in range(0,3):\n",
    "\n",
    "    df1 = pd.DataFrame.from_dict(dfsets[dataset],orient='index')\n",
    "    df1['region'] = dfset_names[dataset]\n",
    "    df1 = df1.reset_index()\n",
    "    df1 = df1.rename(columns=({ 'index' : 'Name'}))\n",
    "    df = df.append(df1,ignore_index=True)\n",
    "\n",
    "# Remove wiki markup from text\n",
    "def unwiki(wiki):\n",
    "        # from https://github.com/lukeorland/wikipedia_sentences_refs/blob/master/mediawiki_article_sentences_refs/wiki2plain.py\n",
    "        \"\"\"\n",
    "       Remove wiki markup from the text.\n",
    "       \"\"\"\n",
    "        wiki = re.sub(r'(?i)\\{\\{IPA(\\-[^\\|\\{\\}]+)*?\\|([^\\|\\{\\}]+)(\\|[^\\{\\}]+)*?\\}\\}', lambda m: m.group(2), wiki)\n",
    "        wiki = re.sub(r'(?i)\\{\\{Lang(\\-[^\\|\\{\\}]+)*?\\|([^\\|\\{\\}]+)(\\|[^\\{\\}]+)*?\\}\\}', lambda m: m.group(2), wiki)\n",
    "        wiki = re.sub(r'\\{\\{[^\\{\\}]+\\}\\}', '', wiki)\n",
    "        wiki = re.sub(r'(?m)\\{\\{[^\\{\\}]+\\}\\}', '', wiki)\n",
    "        wiki = re.sub(r'(?m)\\{\\|[^\\{\\}]*?\\|\\}', '', wiki)\n",
    "        wiki = re.sub(r'(?i)\\[\\[Category:[^\\[\\]]*?\\]\\]', '', wiki)\n",
    "        wiki = re.sub(r'(?i)\\[\\[Image:[^\\[\\]]*?\\]\\]', '', wiki)\n",
    "        wiki = re.sub(r'(?i)\\[\\[File:[^\\[\\]]*?\\]\\]', '', wiki)\n",
    "        wiki = re.sub(r'\\[\\[[^\\[\\]]*?\\|([^\\[\\]]*?)\\]\\]', lambda m: m.group(1), wiki)\n",
    "        wiki = re.sub(r'\\[\\[([^\\[\\]]+?)\\]\\]', lambda m: m.group(1), wiki)\n",
    "        wiki = re.sub(r'\\[\\[([^\\[\\]]+?)\\]\\]', '', wiki)\n",
    "        wiki = re.sub(r'(?i)File:[^\\[\\]]*?', '', wiki)\n",
    "        wiki = re.sub(r'\\[[^\\[\\]]*? ([^\\[\\]]*?)\\]', lambda m: m.group(1), wiki)\n",
    "        wiki = re.sub(r\"''+\", '', wiki)\n",
    "        wiki = re.sub(r'(?m)^\\*$', '', wiki)\n",
    "\n",
    "        return wiki\n",
    "\n",
    "def unhtml(html):\n",
    "        \"\"\"\n",
    "       Remove HTML from the text.\n",
    "       \"\"\"\n",
    "        html = re.sub(r'(?i)&nbsp;', ' ', html)\n",
    "        html = re.sub(r'(?i)<br[ \\\\]*?>', '\\n', html)\n",
    "        html = re.sub(r'(?m)<!--.*?--\\s*>', '', html)\n",
    "        html = re.sub(r'(?i)<ref[^>]*>[^>]*<\\/ ?ref>', '', html)\n",
    "        html = re.sub(r'(?m)<.*?>', '', html)\n",
    "        html = re.sub(r'(?i)&amp;', '&', html)\n",
    "\n",
    "        return html\n",
    "\n",
    "def f(x):\n",
    "    return unhtml(unwiki(x))\n",
    "\n",
    "\n",
    "# Remove category pages from dataset\n",
    "df['text'] = df['text'].apply(f)\n",
    "\n",
    "pages = df[df['type'] == 'page']\n",
    "\n",
    "def f(x):\n",
    "    return re.sub('PAGE', '', x)\n",
    "\n",
    "pages['Name'] = pages['Name'].apply(f)\n",
    "\n",
    "pages = pages[pages['loc'].str.contains(\"//tools.wmflabs.org/wikivoyage/w/poimap2\")==True]\n",
    "\n",
    "StopWords = stopwords.words('english')\n",
    "\n",
    "\n",
    "# Remove other characters and links from text\n",
    "def f(x):\n",
    "    x = re.sub(r'WikiPedia:([\\s\\S]*)','', x)\n",
    "    x = re.sub(r'Dmoz:([\\s\\S]*)','', x)\n",
    "    x = re.sub(r'\\[([\\s\\S]*)\\]','', x)\n",
    "    x = re.sub(r'[0-9]','', x)\n",
    "    #x = re.sub(r'\\=\\=Get in\\=\\=([\\s\\S]*)(?=\\=\\=Get around\\=\\=)','',x)\n",
    "    #x = re.sub(r'\\=\\=Get around\\=\\=([\\s\\S]*)(?=\\=\\=See\\=\\=)','',x)\n",
    "    #x = re.sub(r'\\=\\=Respect\\=\\=([\\s\\S]*)(?=\\=\\=Go next\\=\\=)','',x)\n",
    "    #x = re.sub(r'\\=\\=Stay safe\\=\\=([\\s\\S]*)(?=\\=\\=Go next\\=\\=)','',x)\n",
    "    x = re.sub(r'\\=\\=([\\S ]*)\\=\\=', '', x)\n",
    "    x = re.sub(r'\\=\\=Go next\\=\\=([\\s\\S]*)','', x)\n",
    "    return x\n",
    "\n",
    "pages['regex_text'] = pages['text'].apply(f)\n",
    "\n",
    "pages2 = pages.reset_index()\n",
    "\n",
    "\n",
    "# Remove airports from dataset\n",
    "pages2 = pages2.ix[np.array([i for i,j in enumerate(list(pages2[\"Name\"])) if \"Airport\" not in j])]\n",
    "\n",
    "pages2['len'] = map(lambda x: len(x), pages2['regex_text'])\n",
    "\n",
    "# Remove pages with less than 1000 characters from the dataset\n",
    "pages2 = pages2[pages2['len']> 1000]\n",
    "\n",
    "# Tokenize text\n",
    "features = list(pages2['regex_text'].apply(lambda x: \"\".join(x.lower().splitlines())))\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "features = [tokenizer.tokenize(x) for x in features]\n",
    "\n",
    "# Remove stopwords, short words and wiki references\n",
    "features = [[y for y in x if y not in StopWords and len(y) >2 and 'wiki' not in y] for x in features]\n",
    "labels_list = list(pages2['Name'])\n",
    "\n",
    "# Save to pickle file\n",
    "pickle.dump(features, open( \"features.p\", \"wb\" ) )\n",
    "pickle.dump(labels_list, open( \"labels_list.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Construction & Validation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "import gensim\n",
    "\n",
    "print \"Loading features and labels\"\n",
    "features = pickle.load(open(\"features.p\",\"rb\"))\n",
    "labels_list = pickle.load(open(\"labels_list.p\",\"rb\"))\n",
    "\n",
    "# From https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield LabeledSentence(words=doc,tags=[self.labels_list[idx]])\n",
    "\n",
    "print \"Build vocab\"\n",
    "# Generate vocab\n",
    "it = LabeledLineSentence(features, labels_list)\n",
    "# Use 300 nodes, context window of 20, minimum word count of 5, 11 threads, 0.025 for learning rate\n",
    "# DM model/ CBOW and compute word vectors along with doc vectors\n",
    "\n",
    "model = gensim.models.Doc2Vec(size=300, window=20,\n",
    "                              min_count=5, workers=11,alpha=0.025, min_alpha=0.025,\n",
    "                             dm=1) # use fixed learning rate\n",
    "model.build_vocab(it)\n",
    "\n",
    "print \"Training model...\"\n",
    "for epoch in range(10):\n",
    "    model.train(it)\n",
    "    model.alpha -= 0.002 # decrease the learning rate\n",
    "    model.min_alpha = model.alpha # fix the learning rate, no deca\n",
    "    model.train(it)\n",
    "    print \"Iteration\", epoch+1\n",
    "\n",
    "print \"Training complete\"\n",
    "\n",
    "print \"Saving Model\"\n",
    "\n",
    "model.save(\"doc2vec_dm_NAEUR.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Expand to the entire Wikivoyage dataset\n",
    "\n",
    "2. Restrict results by geographic area\n",
    "    \n",
    "3. Utilize clustering to create groups of destinations\n",
    "\n",
    "4. Pull in additional data, such as places on Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Iteratively tranverse tree structures\n",
    "2. Word2Vec can be used outside of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec/Doc2Vec\n",
    "\n",
    "\n",
    "- Xin Rong: Word2Vec Parameter Learning Explained http://arxiv.org/pdf/1411.2738.pdf\n",
    "- StitchFix: A word is worth a thousand vectors\n",
    "    - Video https://www.youtube.com/watch?v=vkfXBGnDplQ\n",
    "    - Blog Post http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/\n",
    "    \n",
    "- TensorFlow: Vector representations of words https://www.tensorflow.org/versions/master/tutorials/word2vec/index.html\n",
    "- Doc2Vec tutorial using Gensim https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1#.j9zfhhg8i\n",
    "- Deeplearning4j: Word2Vec Tutorial http://deeplearning4j.org/word2vec.html\n",
    "- Rare Technologies: Doc2Vec tutorial http://rare-technologies.com/doc2vec-tutorial/\n",
    "- Distributed Representations of Sentences and Documents http://nbviewer.ipython.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb\n",
    "\n",
    "\n",
    "#### Neural Networks\n",
    "- Neural Networks Demystified \n",
    "    - Video https://www.youtube.com/watch?v=bxe2T-V8XRs\n",
    "    - Github https://github.com/stephencwelch/Neural-Networks-Demystified\n",
    "- Intro to Neural Networks https://www.youtube.com/watch?v=DG5-UyRBQD4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
